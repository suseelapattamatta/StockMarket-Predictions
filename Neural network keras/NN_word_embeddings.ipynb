{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Jupyter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install tensorflow\n",
    "# !pip install keras\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news(news):\n",
    "    _news = news.replace('b\\\"', \"\")\n",
    "    _news = _news.replace('b\\'', \"\")\n",
    "    _news = _news.lower()\n",
    "    _news = re.sub(\"[^a-zA-Z]\", \" \",_news)\n",
    "    _news = re.sub('[\\s]+', ' ', _news)\n",
    "    \n",
    "    tokens = _news.split(\" \")\n",
    "    if \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    #remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    \n",
    "    _news = ' '.join(tokens)    \n",
    "     \n",
    "    return _news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "\n",
    "    data = pd.read_csv(r\"Combined_News_DJIA.csv\")\n",
    "    \n",
    "    dfs = []\n",
    "    data[\"News\"] = \"\"\n",
    "    for i in range(1,25):\n",
    "        col = \"Top\"+str(i)\n",
    "        data[\"News\"] = data[\"News\"] +\" \"+ data[col]\n",
    "    data = data.dropna()\n",
    "    data['PreProcessedNews'] = data['News'].map(process_news)\n",
    "    \n",
    "    data = data[['Date', 'News', 'PreProcessedNews', 'Label']]\n",
    "    \n",
    "    stock_prices = \"upload_DJIA_table.csv\"\n",
    "    stock_data = pd.read_csv(stock_prices)\n",
    "    \n",
    "    print(data.head(2))\n",
    "    print(stock_data.head(2))\n",
    "    \n",
    "    \n",
    "    #merged_dataframe = data.merge(stock_data, how='inner', on='Date')\n",
    "    merged_dataframe = pd.merge(data, stock_data, how='inner', on = 'Date')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    Xy_train = merged_dataframe[:int(len(data)*0.8)]\n",
    "    #Xy_valid = merged_dataframe[int(len(data)*0.6):int(len(data)*0.8)]\n",
    "    Xy_test = merged_dataframe[int(len(data)*0.8):]\n",
    "    \n",
    "    return merged_dataframe, Xy_train, Xy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date                                               News  \\\n",
      "0  2008-08-08   b\"Georgia 'downs two Russian warplanes' as co...   \n",
      "1  2008-08-11   b'Why wont America and Nato help us? If they ...   \n",
      "\n",
      "                                    PreProcessedNews  Label  \n",
      "0  georgia two russian warplane country move brin...      0  \n",
      "1  wont america nato help wont help help iraq bus...      1  \n",
      "         Date          Open          High           Low         Close  \\\n",
      "0  2016-07-01  17924.240234  18002.380859  17916.910156  17949.369141   \n",
      "1  2016-06-30  17712.759766  17930.609375  17711.800781  17929.990234   \n",
      "\n",
      "      Volume     Adj Close  \n",
      "0   82160000  17949.369141  \n",
      "1  133030000  17929.990234  \n"
     ]
    }
   ],
   "source": [
    "news, Xy_train, Xy_test = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Xy_train['PreProcessedNews'].to_frame().reset_index()\n",
    "#X_valid = Xy_valid['PreProcessedNews'].to_frame()\n",
    "#X_train =pd.concat(X_train, X_valid)\n",
    "X_test = Xy_test['PreProcessedNews'].to_frame().reset_index()\n",
    "#Xy_train =pd.concat(Xy_train, Xy_valid) \n",
    "y_train = Xy_train['Label'].to_numpy()\n",
    "#y_valid = Xy_valid['Label'].to_numpy()\n",
    "y_test = Xy_test['Label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "    scores = list()\n",
    "    n_repeats = 5\n",
    "    n_words = Xtest.shape[1]\n",
    "    for i in range(n_repeats):\n",
    "        # define network\n",
    "        \n",
    "        #This is where you modify the network parameters\n",
    "        model = Sequential()\n",
    "        model.add(Dense(75, input_shape=(n_words,), activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # compile network\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        # fit network\n",
    "        model.fit(Xtrain, ytrain,validation_split = 0.25, epochs=10, verbose=2)\n",
    "        # evaluate\n",
    "        loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "        scores.append(acc)\n",
    "        print('%d accuracy: %s' % ((i + 1), acc))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    # create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode training data set\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    # encode training data set\n",
    "    #Xvalid = tokenizer.texts_to_matrix(valid_docs, mode=mode)\n",
    "    # encode testing data set\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 1 0 1]\n",
      "Epoch 1/10\n",
      "38/38 - 2s - loss: 0.7026 - accuracy: 0.5004 - val_loss: 0.6871 - val_accuracy: 0.5617\n",
      "Epoch 2/10\n",
      "38/38 - 2s - loss: 0.3447 - accuracy: 0.9437 - val_loss: 0.7465 - val_accuracy: 0.5139\n",
      "Epoch 3/10\n",
      "38/38 - 2s - loss: 0.0962 - accuracy: 0.9840 - val_loss: 0.8232 - val_accuracy: 0.5466\n",
      "Epoch 4/10\n",
      "38/38 - 2s - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.8832 - val_accuracy: 0.5592\n",
      "Epoch 5/10\n",
      "38/38 - 2s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.9264 - val_accuracy: 0.5416\n",
      "Epoch 6/10\n",
      "38/38 - 2s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.9541 - val_accuracy: 0.5491\n",
      "Epoch 7/10\n",
      "38/38 - 2s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.9863 - val_accuracy: 0.5491\n",
      "Epoch 8/10\n",
      "38/38 - 2s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.0105 - val_accuracy: 0.5592\n",
      "Epoch 9/10\n",
      "38/38 - 2s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.0359 - val_accuracy: 0.5642\n",
      "Epoch 10/10\n",
      "38/38 - 1s - loss: 7.4811e-04 - accuracy: 1.0000 - val_loss: 1.0550 - val_accuracy: 0.5592\n",
      "1 accuracy: 0.5100502371788025\n",
      "Epoch 1/10\n",
      "38/38 - 2s - loss: 0.6996 - accuracy: 0.5080 - val_loss: 0.6880 - val_accuracy: 0.5239\n",
      "Epoch 2/10\n",
      "38/38 - 2s - loss: 0.4524 - accuracy: 0.8976 - val_loss: 0.7176 - val_accuracy: 0.5290\n",
      "Epoch 3/10\n",
      "38/38 - 2s - loss: 0.1533 - accuracy: 0.9765 - val_loss: 0.8046 - val_accuracy: 0.5365\n",
      "Epoch 4/10\n",
      "38/38 - 2s - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.8746 - val_accuracy: 0.5340\n",
      "Epoch 5/10\n",
      "38/38 - 2s - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.9194 - val_accuracy: 0.5390\n",
      "Epoch 6/10\n",
      "38/38 - 2s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.9536 - val_accuracy: 0.5441\n",
      "Epoch 7/10\n",
      "38/38 - 2s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.9829 - val_accuracy: 0.5441\n",
      "Epoch 8/10\n",
      "38/38 - 2s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.0125 - val_accuracy: 0.5340\n",
      "Epoch 9/10\n",
      "38/38 - 2s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.0364 - val_accuracy: 0.5390\n",
      "Epoch 10/10\n",
      "38/38 - 2s - loss: 9.7133e-04 - accuracy: 1.0000 - val_loss: 1.0581 - val_accuracy: 0.5340\n",
      "2 accuracy: 0.5025125741958618\n"
     ]
    }
   ],
   "source": [
    "modes = ['binary', 'count', 'tfidf', 'freq']\n",
    "print(y_train)\n",
    "results = DataFrame()\n",
    "for mode in modes:\n",
    "    Xtrain, Xtest = prepare_data(X_train['PreProcessedNews'], X_test['PreProcessedNews'], mode)\n",
    "    \n",
    "     # prepare data for mode\n",
    "    #print(Xtrain)\n",
    "    results[mode] = evaluate_mode(Xtrain, y_train, Xtest, y_test)\n",
    " # summarize results\n",
    "    print(results.describe())\n",
    " # plot results\n",
    "    results.boxplot()\n",
    "    pyplot.show()\n",
    "\n",
    "#X_train, X_valid, X_test = prepare_data(X_train, X_valid, X_test, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repo contains an introduction to [Jupyter](https://jupyter.org) and [IPython](https://ipython.org).\n",
    "\n",
    "Outline of some basics:\n",
    "\n",
    "* [Notebook Basics](../examples/Notebook/Notebook%20Basics.ipynb)\n",
    "* [IPython - beyond plain python](../examples/IPython%20Kernel/Beyond%20Plain%20Python.ipynb)\n",
    "* [Markdown Cells](../examples/Notebook/Working%20With%20Markdown%20Cells.ipynb)\n",
    "* [Rich Display System](../examples/IPython%20Kernel/Rich%20Output.ipynb)\n",
    "* [Custom Display logic](../examples/IPython%20Kernel/Custom%20Display%20Logic.ipynb)\n",
    "* [Running a Secure Public Notebook Server](../examples/Notebook/Running%20the%20Notebook%20Server.ipynb#Securing-the-notebook-server)\n",
    "* [How Jupyter works](../examples/Notebook/Multiple%20Languages%2C%20Frontends.ipynb) to run code in different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get this tutorial and run it on your laptop:\n",
    "\n",
    "    git clone https://github.com/ipython/ipython-in-depth\n",
    "\n",
    "Install IPython and Jupyter:\n",
    "\n",
    "with [conda](https://www.anaconda.com/download):\n",
    "\n",
    "    conda install ipython jupyter\n",
    "\n",
    "with pip:\n",
    "\n",
    "    # first, always upgrade pip!\n",
    "    pip install --upgrade pip\n",
    "    pip install --upgrade ipython jupyter\n",
    "\n",
    "Start the notebook in the tutorial directory:\n",
    "\n",
    "    cd ipython-in-depth\n",
    "    jupyter notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
